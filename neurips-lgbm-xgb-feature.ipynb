{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":12207625,"sourceType":"datasetVersion","datasetId":7690162},{"sourceId":12330396,"sourceType":"datasetVersion","datasetId":7709869},{"sourceId":248128822,"sourceType":"kernelVersion"},{"sourceId":458385,"sourceType":"modelInstanceVersion","modelInstanceId":371577,"modelId":392477}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:42:45.770929Z","iopub.execute_input":"2025-07-09T07:42:45.771293Z","iopub.status.idle":"2025-07-09T07:42:49.822304Z","shell.execute_reply.started":"2025-07-09T07:42:45.771268Z","shell.execute_reply":"2025-07-09T07:42:49.82109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers rdkit pandas tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:42:49.824477Z","iopub.execute_input":"2025-07-09T07:42:49.824809Z","iopub.status.idle":"2025-07-09T07:42:53.76226Z","shell.execute_reply.started":"2025-07-09T07:42:49.824778Z","shell.execute_reply":"2025-07-09T07:42:53.760991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom transformers import AutoTokenizer, AutoModel\nimport gc\nimport pickle\nimport torch\nfrom tqdm import tqdm\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\n\nimport networkx as nx\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem import Descriptors\nfrom rdkit.Chem import rdmolops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:42:53.763852Z","iopub.execute_input":"2025-07-09T07:42:53.76485Z","iopub.status.idle":"2025-07-09T07:42:53.772416Z","shell.execute_reply.started":"2025-07-09T07:42:53.764811Z","shell.execute_reply":"2025-07-09T07:42:53.771271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n    SEED = 42\n    FOLDS = 5\n    PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\n\ntrain = pd.read_csv(CFG.PATH + 'train.csv')\ntest = pd.read_csv(CFG.PATH + 'test.csv')\n\ndef make_smile_canonical(smile): # To avoid duplicates, for example: canonical '*C=C(*)C' == '*C(=C*)C'\n    try:\n        mol = Chem.MolFromSmiles(smile)\n        canon_smile = Chem.MolToSmiles(mol, canonical=True)\n        return canon_smile\n    except:\n        return np.nan\n\ntrain['SMILES'] = train['SMILES'].apply(lambda s: make_smile_canonical(s))\ntest['SMILES'] = test['SMILES'].apply(lambda s: make_smile_canonical(s))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:42:53.774565Z","iopub.execute_input":"2025-07-09T07:42:53.774861Z","iopub.status.idle":"2025-07-09T07:42:59.753713Z","shell.execute_reply.started":"2025-07-09T07:42:53.774838Z","shell.execute_reply":"2025-07-09T07:42:59.75278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://www.kaggle.com/datasets/minatoyukinaxlisa/tc-smiles\ndata_tc = pd.read_csv('/kaggle/input/tc-smiles/Tc_SMILES.csv')\ndata_tc = data_tc.rename(columns={'TC_mean': 'Tc'})\n\n# https://springernature.figshare.com/articles/dataset/dataset_with_glass_transition_temperature/24219958?file=42507037\ndata_tg2 = pd.read_csv('/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv', usecols=['SMILES', 'Tg (C)'])\ndata_tg2 = data_tg2.rename(columns={'Tg (C)': 'Tg'})\n\n# https://www.sciencedirect.com/science/article/pii/S2590159123000377#ec0005\ndata_tg3 = pd.read_excel('/kaggle/input/smiles-extra-data/data_tg3.xlsx')\ndata_tg3 = data_tg3.rename(columns={'Tg [K]': 'Tg'})\ndata_tg3['Tg'] = data_tg3['Tg'] - 273.15\n\n# https://github.com/Duke-MatSci/ChemProps\ndata_dnst = pd.read_excel('/kaggle/input/smiles-extra-data/data_dnst1.xlsx')\ndata_dnst = data_dnst.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\ndata_dnst['SMILES'] = data_dnst['SMILES'].apply(lambda s: make_smile_canonical(s))\ndata_dnst = data_dnst[(data_dnst['SMILES'].notnull())&(data_dnst['Density'].notnull())&(data_dnst['Density'] != 'nylon')]\ndata_dnst['Density'] = data_dnst['Density'].astype('float64')\ndata_dnst['Density'] -= 0.118\n\ndataset1 = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\")\ndataset3 = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\")\ndataset4 = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\")\ndataset1 = dataset1.rename(columns={'TC_mean': 'Tc'})\n\n\ndef add_extra_data(df_train, df_extra, target):\n    n_samples_before = len(df_train[df_train[target].notnull()])\n    \n    df_extra['SMILES'] = df_extra['SMILES'].apply(lambda s: make_smile_canonical(s))\n    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n\n    # Make priority target value from competition's df\n    for smile in df_train[df_train[target].notnull()]['SMILES'].tolist():\n        if smile in cross_smiles:\n            cross_smiles.remove(smile)\n\n    # Imput missing values for competition's SMILES\n    for smile in cross_smiles:\n        df_train.loc[df_train['SMILES']==smile, target] = df_extra[df_extra['SMILES']==smile][target].values[0]\n    \n    df_train = pd.concat([df_train, df_extra[df_extra['SMILES'].isin(unique_smiles_extra)]], axis=0).reset_index(drop=True)\n\n    n_samples_after = len(df_train[df_train[target].notnull()])\n    print(f'\\nFor target \"{target}\" added {n_samples_after-n_samples_before} new samples!')\n    print(f'New unique SMILES: {len(unique_smiles_extra)}')\n    return df_train\n\ntrain = add_extra_data(train, data_tc, 'Tc')\ntrain = add_extra_data(train, data_tg2, 'Tg')\ntrain = add_extra_data(train, data_tg3, 'Tg')\ntrain = add_extra_data(train, data_dnst, 'Density')\ntrain = add_extra_data(train, dataset1, 'Tc')\ntrain = add_extra_data(train, dataset3, 'Tg')\ntrain = add_extra_data(train, dataset4, 'FFV')\n\nprint('\\n'*3, '--- SMILES for training ---', )\nfor t in CFG.TARGETS:\n    print(f'\"{t}\": {len(train[train[t].notnull()])}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:42:59.755131Z","iopub.execute_input":"2025-07-09T07:42:59.755413Z","iopub.status.idle":"2025-07-09T07:43:02.773224Z","shell.execute_reply.started":"2025-07-09T07:42:59.755384Z","shell.execute_reply":"2025-07-09T07:43:02.772348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel_dir = \"/kaggle/input/bert_smile/pytorch/default/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModel.from_pretrained(model_dir)\n\nmodel.eval()  # ÌèâÍ∞Ä Î™®ÎìúÎ°ú ÏÑ§Ï†ï (ÎìúÎ°≠ÏïÑÏõÉ Îì± ÎπÑÌôúÏÑ±Ìôî)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:43:02.774101Z","iopub.execute_input":"2025-07-09T07:43:02.774354Z","iopub.status.idle":"2025-07-09T07:43:02.862444Z","shell.execute_reply.started":"2025-07-09T07:43:02.774333Z","shell.execute_reply":"2025-07-09T07:43:02.86167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)  # Î™®Îç∏ GPUÎ°ú\n\ndef smiles_to_embedding(smiles, tokenizer, model):\n    try:\n        # ÏûÖÎ†•ÏùÑ GPUÎ°ú\n        inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n        inputs = {key: val.to(device) for key, val in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        last_hidden = outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n        pooled = last_hidden.mean(dim=1).squeeze()  # [hidden_dim]\n        return pooled.cpu().numpy()  # GPU ‚Üí CPU ‚Üí NumPy\n    except Exception as e:\n        print(f\"Error in SMILES: {smiles} - {e}\")\n        return [0.0] * 768  # ÏóêÎü¨ Ïãú 0 Î≤°ÌÑ∞\n\n# Train SMILES ÏûÑÎ≤†Îî©\ntrain_embeddings = []\nfor smi in tqdm(train[\"SMILES\"], desc=\"Embedding train SMILES\"):\n    emb = smiles_to_embedding(smi, tokenizer, model)\n    train_embeddings.append(emb)\n\ntrain_emb_df = pd.DataFrame(train_embeddings, columns=[f\"smiles_emb_{i}\" for i in range(768)])\ntrain = pd.concat([train.reset_index(drop=True), train_emb_df], axis=1)\n\n# Test SMILES ÏûÑÎ≤†Îî©\ntest_embeddings = []\nfor smi in tqdm(test[\"SMILES\"], desc=\"Embedding test SMILES\"):\n    emb = smiles_to_embedding(smi, tokenizer, model)\n    test_embeddings.append(emb)\n\ntest_emb_df = pd.DataFrame(test_embeddings, columns=[f\"smiles_emb_{i}\" for i in range(768)])\ntest = pd.concat([test.reset_index(drop=True), test_emb_df], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:43:02.863235Z","iopub.execute_input":"2025-07-09T07:43:02.863465Z","iopub.status.idle":"2025-07-09T07:51:44.442684Z","shell.execute_reply.started":"2025-07-09T07:43:02.863448Z","shell.execute_reply":"2025-07-09T07:51:44.441691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.decomposition import PCA\n\n# 1. SMILES ÏûÑÎ≤†Îî© Ïª¨Îüº Ï∂îÏ∂ú\nembedding_cols = [col for col in train.columns if col.startswith('smiles_emb_')]\n\n# 2. TrainÏóêÏÑú PCA ÌïôÏäµ\nX_emb_train = train[embedding_cols].values\npca = PCA(n_components=0.9, svd_solver='full')\nX_pca_train = pca.fit_transform(X_emb_train)\n\n# 3. TestÏóê PCA Ï†ÅÏö©\nX_emb_test = test[embedding_cols].values\nX_pca_test = pca.transform(X_emb_test)\n\n# 4. Ïª¨Îüº Ïù¥Î¶Ñ ÏÉùÏÑ±\npca_cols = [f'pca_smiles_emb_{i}' for i in range(X_pca_train.shape[1])]\n\n# 5. DataFrame ÏÉùÏÑ±\ndf_pca_train = pd.DataFrame(X_pca_train, columns=pca_cols, index=train.index)\ndf_pca_test = pd.DataFrame(X_pca_test, columns=pca_cols, index=test.index)\n\n# 6. Í∏∞Ï°¥ ÏûÑÎ≤†Îî© Ïª¨Îüº Ï†úÍ±∞\ntrain.drop(columns=embedding_cols, inplace=True)\ntest.drop(columns=embedding_cols, inplace=True)\n\n# 7. PCA Ïª¨Îüº Ï∂îÍ∞Ä\ntrain = pd.concat([train, df_pca_train], axis=1)\ntest = pd.concat([test, df_pca_test], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:51:44.443803Z","iopub.execute_input":"2025-07-09T07:51:44.444083Z","iopub.status.idle":"2025-07-09T07:51:45.259152Z","shell.execute_reply.started":"2025-07-09T07:51:44.444055Z","shell.execute_reply":"2025-07-09T07:51:45.25848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nuseless_cols = [\n    # Nan data\n    'BCUT2D_MWHI',\n    'BCUT2D_MWLOW',\n    'BCUT2D_CHGHI',\n    'BCUT2D_CHGLO',\n    'BCUT2D_LOGPHI',\n    'BCUT2D_LOGPLOW',\n    'BCUT2D_MRHI',\n    'BCUT2D_MRLOW',\n\n    # Constant data\n    'NumRadicalElectrons',\n    'SMR_VSA8',\n    'SlogP_VSA9',\n    'fr_barbitur',\n    'fr_benzodiazepine',\n    'fr_dihydropyridine',\n    'fr_epoxide',\n    'fr_isothiocyan',\n    'fr_lactam',\n    'fr_nitroso',\n    'fr_prisulfonamd',\n    'fr_thiocyan',\n\n    # High correlated data >0.95\n    'MaxEStateIndex',\n    'HeavyAtomMolWt',\n    'ExactMolWt',\n    'NumValenceElectrons',\n    'Chi0',\n    'Chi0n',\n    'Chi0v',\n    'Chi1',\n    'Chi1n',\n    'Chi1v',\n    'Chi2n',\n    'Kappa1',\n    'LabuteASA',\n    'HeavyAtomCount',\n    'MolMR',\n    'Chi3n',\n    'BertzCT',\n    'Chi2v',\n    'Chi4n',\n    'HallKierAlpha',\n    'Chi3v',\n    'Chi4v',\n    'MinAbsPartialCharge',\n    'MinPartialCharge',\n    'MaxAbsPartialCharge',\n    'FpDensityMorgan2',\n    'FpDensityMorgan3',\n    'Phi',\n    'Kappa3',\n    'fr_nitrile',\n    'SlogP_VSA6',\n    'NumAromaticCarbocycles',\n    'NumAromaticRings',\n    'fr_benzene',\n    'VSA_EState6',\n    'NOCount',\n    'fr_C_O',\n    'fr_C_O_noCOO',\n    'NumHDonors',\n    'fr_amide',\n    'fr_Nhpyrrole',\n    'fr_phenol',\n    'fr_phenol_noOrthoHbond',\n    'fr_COO2',\n    'fr_halogen',\n    'fr_diazo',\n    'fr_nitro_arom',\n    'fr_phos_ester',\n    #low shap\n    'NumBridgeheadAtoms', 'NumSaturatedCarbocycles', 'NumSaturatedRings',\n    'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA3', 'PEOE_VSA4',\n    'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_Ar_COO', 'fr_Ar_NH',\n    'fr_COO', 'fr_Imine', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2',\n    'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_amidine', 'fr_azo', 'fr_furan',\n    'fr_imidazole', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactone', 'fr_methoxy',\n    'fr_morpholine',  'fr_nitro_arom_nonortho', 'fr_piperdine',\n    'fr_priamide', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd',\n    'fr_sulfone', 'fr_thiazole', 'fr_urea'\n]\ndef count_atoms(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    counts = {'num_C': 0, 'num_c': 0, 'num_O': 0, 'num_N': 0, 'num_F': 0, 'num_Cl': 0}\n    if mol is None:\n        return counts\n\n    for atom in mol.GetAtoms():\n        symbol = atom.GetSymbol()\n        if symbol == 'C':\n            if atom.GetIsAromatic():\n                counts['num_c'] += 1\n            else:\n                counts['num_C'] += 1\n        elif symbol == 'Cl':\n            counts['num_Cl'] += 1\n        elif symbol in ['O', 'N', 'F']:\n            counts[f'num_{symbol}'] += 1\n    return counts\n\n\ndef compute_all_descriptors(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return [None] * len(desc_names)\n    return [desc[1](mol) for desc in Descriptors.descList if desc[0] not in useless_cols]\n\ndef compute_graph_features(smiles, graph_feats):\n    mol = Chem.MolFromSmiles(smiles)\n    adj = rdmolops.GetAdjacencyMatrix(mol)\n    G = nx.from_numpy_array(adj)\n\n    graph_feats['graph_diameter'].append(nx.diameter(G) if nx.is_connected(G) else 0)\n    graph_feats['avg_shortest_path'].append(nx.average_shortest_path_length(G) if nx.is_connected(G) else 0)\n    graph_feats['num_cycles'].append(len(list(nx.cycle_basis(G))))\n\ndef preprocessing(df):\n    desc_names = [desc[0] for desc in Descriptors.descList if desc[0] not in useless_cols]\n    descriptors = [compute_all_descriptors(smi) for smi in df['SMILES'].to_list()]\n\n    # Í∑∏ÎûòÌîÑ ÌäπÏÑ± Ï∂îÏ∂ú\n    graph_feats = {'graph_diameter': [], 'avg_shortest_path': [], 'num_cycles': []}\n    for smile in df['SMILES']:\n         compute_graph_features(smile, graph_feats)\n\n    # ÏõêÏûê Í∞úÏàò Í≥ÑÏÇ∞\n    atom_counts = [count_atoms(smi) for smi in df['SMILES']]\n    atom_df = pd.DataFrame(atom_counts)\n    # Í≤∞Ìï©\n    result = pd.concat(\n        [\n            pd.DataFrame(descriptors, columns=desc_names),\n            pd.DataFrame(graph_feats),\n            atom_df\n        ],\n        axis=1\n    )\n    # ÌõÑÏ≤òÎ¶¨ Î∞è ÌååÏÉùÎ≥ÄÏàò\n    result = result.replace([-np.inf, np.inf], np.nan)\n    eps = 1e-6\n\n    result[\"MaxPartialCharge_mul_EState_VSA6\"] = result[\"MaxPartialCharge\"] * result[\"EState_VSA6\"]\n    result[\"MaxPartialCharge_mul_SMR_VSA9\"] = result[\"MaxPartialCharge\"] * result[\"SMR_VSA9\"]\n    result[\"MaxPartialCharge_mul_RingCount\"] = result[\"MaxPartialCharge\"] * result[\"RingCount\"]\n    result[\"MaxPartialCharge_mul_num_cycles\"] = result[\"MaxPartialCharge\"] * result[\"num_cycles\"]\n    result[\"MaxPartialCharge_div_NumHAcceptors\"] = result[\"MaxPartialCharge\"] / (result[\"NumHAcceptors\"] + eps)\n    result[\"MaxPartialCharge_div_MinEStateIndex\"] = result[\"MaxPartialCharge\"] / (result[\"MinEStateIndex\"] + eps)\n    result[\"MaxPartialCharge_div_SlogP_VSA12\"] = result[\"MaxPartialCharge\"] / (result[\"SlogP_VSA12\"] + eps)\n    result[\"MaxPartialCharge_div_PEOE_VSA6\"] = result[\"MaxPartialCharge\"] / (result[\"PEOE_VSA6\"] + eps)\n    result[\"MaxPartialCharge_mul_SlogP_VSA10\"] = result[\"MaxPartialCharge\"] * result[\"SlogP_VSA10\"]\n    result[\"MaxPartialCharge_mul_AvgIpc\"] = result[\"MaxPartialCharge\"] * result[\"AvgIpc\"]\n    result[\"NumAromaticHeterocycles_div_NumHeteroatoms\"]=result[\"NumAromaticHeterocycles\"]/(result[\"NumHeteroatoms\"]+eps)\n    result[\"fr_unbrch_alkane_div_MolWt \"]=result[\"fr_unbrch_alkane\"]/(result[\"MolWt\"]+eps)\n    result[\"fr_ester_div_MaxPartialCharge\"] = result[\"fr_ester\"] / (result[\"MaxPartialCharge\"] + eps)\n    result[\"avg_shortest_path_div_MaxPartialCharge\"] = result[\"avg_shortest_path\"] / (result[\"MaxPartialCharge\"] + eps)\n    result[\"graph_diameter_div_MaxPartialCharge\"] = result[\"graph_diameter\"] / (result[\"MaxPartialCharge\"] + eps)\n    result[\"VSA_EState8_div_MaxPartialCharge\"] = result[\"VSA_EState8\"] / (result[\"MaxPartialCharge\"] + eps)\n    result[\"EState_VSA5_div_MaxPartialCharge\"] = result[\"EState_VSA5\"] / (result[\"MaxPartialCharge\"] + eps)\n    result[\"PEOE_VSA14_div_graph_diameter\"] = result[\"PEOE_VSA14\"] / (result[\"graph_diameter\"] + eps)\n    result[\"BalabanJ_mul_TPSA\"] = result[\"BalabanJ\"] * result[\"TPSA\"]\n    result[\"qed_mul_SMR_VSA5\"] = result[\"qed\"] * result[\"SMR_VSA5\"]\n    result[\"VSA_EState7_div_MolWt\"] = result[\"VSA_EState7\"] / (result[\"MolWt\"] + eps)\n    result[\"SMR_VSA10_div_MolWt\"] = result[\"SMR_VSA10\"] / (result[\"MolWt\"] + eps)\n    result[\"SlogP_VSA12_div_MolWt\"] = result[\"SlogP_VSA12\"] / (result[\"MolWt\"] + eps)\n    result[\"SMR_VSA10_div_fr_unbrch_alkane\"] = result[\"SMR_VSA10\"] / (result[\"fr_unbrch_alkane\"] + eps)\n    result[\"qed_mul_TPSA\"] = result[\"qed\"] * result[\"TPSA\"]\n    result[\"PEOE_VSA14_div_fr_unbrch_alkane\"] = result[\"PEOE_VSA14\"] / (result[\"fr_unbrch_alkane\"] + eps)\n    result[\"PEOE_VSA14_mul_AvgIpc\"] = result[\"PEOE_VSA14\"] * result[\"AvgIpc\"]\n    result[\"SMR_VSA5_div_MolWt\"] = result[\"SMR_VSA5\"] / (result[\"MolWt\"] + eps)\n    result[\"PEOE_VSA14_div_SlogP_VSA7\"] = result[\"PEOE_VSA14\"] / (result[\"SlogP_VSA7\"] + eps)\n    result[\"VSA_EState7_div_SPS\"] = result[\"VSA_EState7\"] / (result[\"SPS\"] + eps)\n    result[\"SlogP_VSA5_mul_FpDensityMorgan1\"] = result[\"SlogP_VSA5\"] * result[\"FpDensityMorgan1\"]\n    result[\"VSA_EState8_div_PEOE_VSA5\"] = result[\"VSA_EState8\"] / (result[\"PEOE_VSA5\"] + eps)\n    result['MaxPartialCharge_mul_num_cycles_div_fr_oxazole'] = (\n        result['MaxPartialCharge'] * result['num_cycles']\n    ) / (result['fr_oxazole'] + eps)\n    result['SMR_VSA5_div_MolWt_div_fr_nitro'] = (\n        result['SMR_VSA5'] / (result['MolWt'] + eps)\n    ) / (result['fr_nitro'] + eps)\n    result['fr_unbrch_alkane_div_MolWt_div_EState_VSA11'] = (\n        result['fr_unbrch_alkane'] / (result['MolWt'] + eps)\n    ) / (result['EState_VSA11'] + eps)\n    result['PEOE_VSA14_div_graph_diameter_div_BalabanJ'] = (\n        result['PEOE_VSA14'] / (result['graph_diameter'] + eps)\n    ) / (result['BalabanJ'] + eps)\n    result['VSA_EState7_div_SPS_div_PEOE_VSA14'] = (\n        result['VSA_EState7'] / (result['SPS'] + eps)\n    ) / (result['PEOE_VSA14'] + eps)\n    return result\n\ntrain = pd.concat([train, preprocessing(train)], axis=1)\ntest = pd.concat([test, preprocessing(test)], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:57:12.245277Z","iopub.execute_input":"2025-07-09T07:57:12.245674Z","iopub.status.idle":"2025-07-09T08:00:07.19757Z","shell.execute_reply.started":"2025-07-09T07:57:12.245648Z","shell.execute_reply":"2025-07-09T08:00:07.196256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_features = train.columns[7:].tolist()\nfeatures = {}\nfor target in CFG.TARGETS:\n    const_descs = []\n    for col in train.columns.drop(CFG.TARGETS):\n        if train.loc[train[target].notnull(), col].nunique() == 1:\n            const_descs.append(col)\n    features[target] = [f for f in all_features if f not in const_descs]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T08:00:07.199574Z","iopub.execute_input":"2025-07-09T08:00:07.19997Z","iopub.status.idle":"2025-07-09T08:00:07.708846Z","shell.execute_reply.started":"2025-07-09T08:00:07.19994Z","shell.execute_reply":"2025-07-09T08:00:07.706042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T08:00:07.710289Z","iopub.execute_input":"2025-07-09T08:00:07.710662Z","iopub.status.idle":"2025-07-09T08:00:07.717094Z","shell.execute_reply.started":"2025-07-09T08:00:07.710607Z","shell.execute_reply":"2025-07-09T08:00:07.716173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom lightgbm import early_stopping, reset_parameter\n\nlast_model_lgb_dict = {}\nlast_model_xgb_dict = {}\n\n# Îü¨ÎãùÎ†àÏù¥Ìä∏ ÎîîÏºÄÏù¥ Ìï®Ïàò\ndef lr_decay(current_round, total_rounds=10000, start=0.02, end=0.005):\n    lr = start - (start - end) * (current_round / total_rounds)\n    return max(lr, end)\n\ndef lgb_lr_decay(current_round, target):\n    if target == 'FFV':\n        start = 0.02\n        end = 0.005\n        total = 10000\n        lr = start - (start - end) * (current_round / total)\n        return max(lr, end)\n    else:\n        return 0.01\n\n# MAE Í≥ÑÏÇ∞ Ìï®Ïàò\ndef mae(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))\n\n# ÏµúÏ†Å Í∞ÄÏ§ëÏπò ÌÉêÏÉâ Ìï®Ïàò\ndef find_best_weight(y_true, pred_lgb, pred_xgb):\n    best_mae = float('inf')\n    best_weight = 0.5\n    for w in np.linspace(0, 1, 101):\n        blended = w * pred_lgb + (1 - w) * pred_xgb\n        current_mae = mae(y_true, blended)\n        if current_mae < best_mae:\n            best_mae = current_mae\n            best_weight = w\n    return best_weight, best_mae\n\n# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\nbase_params = {\n    'device_type': 'cpu',\n    'n_estimators': 1000000,\n    'objective': 'regression_l1',\n    'metric': 'mae',\n    'verbosity': -1,\n    'num_leaves': 30,\n    'min_data_in_leaf': 2,\n    'learning_rate': 0.01,  # Í∏∞Î≥∏Í∞í, Ïã§Ï†ú Îü¨ÎãùÎ†àÏù¥Ìä∏Îäî callbackÏóêÏÑú Ï°∞Ï†à\n    'max_bin': 512,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n    'lambda_l1': 2,\n    'lambda_l2': 3,\n}\n\nxgb_params = {\n    #'tree_method': 'gpu_hist',\n    'n_estimators': 1000000,\n    'objective': 'reg:squarederror',\n    'learning_rate': 0.01,\n    'max_depth': 4,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 1,\n    'verbosity': 0,\n}\n\n# Í≤∞Í≥º Ï†ÄÏû•Ïö© dict\noof_results = {}\ntest_results = {}\nensemble_weights = {}\n\noof_lgb_dict = {}\noof_xgb_dict = {}\noof_ens_dict = {}\n\n# Í≤∞Í≥º Ï†ÄÏû• dict Ï¥àÍ∏∞Ìôî\noof_results = {}\ntest_results = {}\nensemble_weights = {}\n\noof_lgb_dict = {}\noof_xgb_dict = {}\noof_ens_dict = {}\n\nfor target in CFG.TARGETS:\n    print(f'\\n‚ñ∂ TARGET: {target}')\n    \n    train_part = train[train[target].notnull()].reset_index(drop=True)\n    \n    \n    test[target] = 0\n    features_list = features[target]\n\n    oof_lgb = np.zeros(len(train_part))\n    oof_xgb = np.zeros(len(train_part))\n    test_preds_lgb = np.zeros(len(test))\n    test_preds_xgb = np.zeros(len(test))\n    scores_lgb, scores_xgb = [], []\n\n    kf = KFold(n_splits=CFG.FOLDS, shuffle=True, random_state=CFG.SEED)\n\n    last_model_lgb = None\n    last_model_xgb = None\n\n    for i, (trn_idx, val_idx) in enumerate(kf.split(train_part, train_part[target])):\n        print(f'\\n--- Fold {i+1} ---')\n        X_trn = train_part.loc[trn_idx, features_list]\n        y_trn = train_part.loc[trn_idx, target]\n        X_val = train_part.loc[val_idx, features_list]\n        y_val = train_part.loc[val_idx, target]\n\n        X_trn = np.clip(X_trn, -1e6, 1e6)\n        X_val = np.clip(X_val, -1e6, 1e6)\n\n        # LightGBM\n        model_lgb = lgb.LGBMRegressor(**base_params)\n        callbacks = [\n            lgb.early_stopping(300, verbose=False),\n            lgb.reset_parameter(learning_rate=lambda cur_round: lgb_lr_decay(cur_round, target))\n        ]\n        model_lgb.fit(\n            X_trn, y_trn,\n            eval_set=[(X_val, y_val)],\n            callbacks=callbacks,\n       \n        )\n        val_preds_lgb = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration_)\n        print(f'LightGBM Fold {i+1} MAE: {mae(y_val, val_preds_lgb):.5f}')\n        oof_lgb[val_idx] = val_preds_lgb\n        test_preds_lgb += model_lgb.predict(test[features_list]) / CFG.FOLDS\n        scores_lgb.append(mae(y_val, val_preds_lgb))\n\n        # ÎßàÏßÄÎßâ fold LightGBM Î™®Îç∏ Ï†ÄÏû•\n        if i == CFG.FOLDS - 1:\n            last_model_lgb_dict[target] = model_lgb\n\n        # XGBoost\n        xgb_callbacks = []\n        if target == 'FFV':\n            xgb_callbacks = [xgb.callback.LearningRateScheduler(lambda r: lgb_lr_decay(r, target))]\n        model_xgb = xgb.XGBRegressor(**xgb_params)\n        model_xgb.fit(\n            X_trn, y_trn,\n            eval_set=[(X_val, y_val)],\n            early_stopping_rounds=300,\n            verbose=False,\n            callbacks=xgb_callbacks\n        )\n        val_preds_xgb = model_xgb.predict(X_val, iteration_range=(0, model_xgb.best_iteration))\n        print(f'XGBoost Fold {i+1} MAE: {mae(y_val, val_preds_xgb):.5f}')\n        oof_xgb[val_idx] = val_preds_xgb\n        test_preds_xgb += model_xgb.predict(test[features_list]) / CFG.FOLDS\n        scores_xgb.append(mae(y_val, val_preds_xgb))\n\n        # ÎßàÏßÄÎßâ fold XGBoost Î™®Îç∏ Ï†ÄÏû•\n        if i == CFG.FOLDS - 1:\n            last_model_xgb_dict[target] = model_xgb\n\n    y_true = train_part[target].values\n    best_w, best_mae = find_best_weight(y_true, oof_lgb, oof_xgb)\n    ensemble_weights[target] = best_w\n\n    oof_blend = best_w * oof_lgb + (1 - best_w) * oof_xgb\n    test_blend = best_w * test_preds_lgb + (1 - best_w) * test_preds_xgb\n\n    oof_lgb_dict[target] = oof_lgb\n    oof_xgb_dict[target] = oof_xgb\n    oof_ens_dict[target] = oof_blend\n\n    oof_results[target] = oof_blend\n    test_results[target] = test_blend\n\n    print(f'\\nüìå LightGBM MAE: {np.mean(scores_lgb):.5f}')\n    print(f'üìå XGBoost  MAE: {np.mean(scores_xgb):.5f}')\n    print(f'üîç Best Weight (LGB): {best_w:.2f} ‚Üí Blended MAE: {best_mae:.5f}')\n    print('-'*40)\n\n# OOF Î∞è ÌÖåÏä§Ìä∏ Í≤∞Í≥º Ï†ÄÏû•\noof_df = pd.DataFrame({'id': train['id']})\ntest_df = pd.DataFrame({'id': test['id']})\n\nfor target in CFG.TARGETS:\n    oof_col = np.full(len(train), np.nan)\n    nonnull_idx = train[train[target].notnull()].index\n    oof_col[nonnull_idx] = oof_results[target]\n\n    oof_df[target] = oof_col\n    test_df[target] = test_results[target]\n\noof_df.to_csv('oof_ensemble.csv', index=False)\ntest_df.to_csv('test_ensemble.csv', index=False)\n\nprint('\\n‚úÖ ÏïôÏÉÅÎ∏î ÏôÑÎ£å Î∞è ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å')\n\n\n\"\"\"\n‚ñ∂ TARGET: Tg\nüìå LightGBM MAE: 36.31066\nüìå XGBoost  MAE: 35.90509\nüîç Best Weight (LGB): 0.39 ‚Üí Blended MAE: 35.73759\n----------------------------------------\n\n\n‚ñ∂ TARGET: FFV\nüìå LightGBM MAE: 0.00648\nüìå XGBoost  MAE: 0.00929\nüîç Best Weight (LGB): 1.00 ‚Üí Blended MAE: 0.00648\n----------------------------------------\n\n\n‚ñ∂ TARGET: Tc\nüìå LightGBM MAE: 0.03016\nüìå XGBoost  MAE: 0.03349\nüîç Best Weight (LGB): 1.00 ‚Üí Blended MAE: 0.03015\n----------------------------------------\n\n\n‚ñ∂ TARGET: Density\nüìå LightGBM MAE: 0.03904\nüìå XGBoost  MAE: 0.04353\nüîç Best Weight (LGB): 0.94 ‚Üí Blended MAE: 0.03902\n----------------------------------------\n\n\n‚ñ∂ TARGET: Rg\nüìå LightGBM MAE: 1.69317\nüìå XGBoost  MAE: 1.66246\nüîç Best Weight (LGB): 0.06 ‚Üí Blended MAE: 1.66245\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T08:00:07.720134Z","iopub.execute_input":"2025-07-09T08:00:07.720543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import shap\nimport numpy as np\n\ncommon_low_features_all_targets = None  # Î™®Îì† target Í≥µÌÜµ ÌïòÏúÑ ÌîºÏ≤ò ÍµêÏßëÌï© Ï†ÄÏû•Ïö©\n\nfor target in CFG.TARGETS:\n    print(f\"\\nüîç SHAP Î∂ÑÏÑù ÏãúÏûë: {target}\")\n\n    train_part = train[train[target].notnull()].reset_index(drop=True)\n    features_list = features[target]\n    X_sample = train_part[features_list].copy()\n    X_sample = X_sample.replace([np.inf, -np.inf], np.nan).fillna(0)\n    X_sample = np.clip(X_sample, -1e6, 1e6)\n\n    model_lgb = last_model_lgb_dict[target]\n    explainer_lgb = shap.Explainer(model_lgb)\n    shap_values_lgb = explainer_lgb(X_sample)\n    shap_mean_lgb = np.abs(shap_values_lgb.values).mean(axis=0)\n\n    model_xgb = last_model_xgb_dict[target]\n    explainer_xgb = shap.Explainer(model_xgb)\n    shap_values_xgb = explainer_xgb(X_sample)\n    shap_mean_xgb = np.abs(shap_values_xgb.values).mean(axis=0)\n\n    low_idx_lgb = np.argsort(shap_mean_lgb)[:100]\n    low_idx_xgb = np.argsort(shap_mean_xgb)[:100]\n\n    common_low_idx = np.intersect1d(low_idx_lgb, low_idx_xgb)\n    common_low_features = np.array(features_list)[common_low_idx]\n\n    print(f\"{target} Í≥µÌÜµ ÌïòÏúÑ 100Í∞ú ÌîºÏ≤ò Ïàò: {len(common_low_features)}\")\n\n    if common_low_features_all_targets is None:\n        common_low_features_all_targets = set(common_low_features)\n    else:\n        common_low_features_all_targets = common_low_features_all_targets.intersection(common_low_features)\n\nprint(f\"\\n=== Î™®Îì† targetÏóêÏÑú Í≥µÌÜµÏúºÎ°ú ÌïòÏúÑ 100Í∞úÏóê Ìè¨Ìï®Îêú ÌîºÏ≤ò Î™©Î°ù (Í∞úÏàò: {len(common_low_features_all_targets)}) ===\")\nprint(sorted(common_low_features_all_targets))\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor t in CFG.TARGETS:\n    y_true = train[train[t].notnull()][t].values\n    \n    # Í∞ÅÍ∞Å Î™®Îç∏Ïùò OOF ÏòàÏ∏° (Í∞ÅÍ∞Å ÎîïÏÖîÎÑàÎ¶¨Î°ú Ï†ÄÏû•ÌñàÎã§Í≥† Í∞ÄÏ†ï)\n    lgb_preds_oof = oof_lgb_dict[t]\n    xgb_preds_oof = oof_xgb_dict[t]\n    ens_preds_oof = oof_ens_dict[t]\n\n    line_min = min(y_true.min(), lgb_preds_oof.min(), xgb_preds_oof.min(), ens_preds_oof.min())\n    line_max = max(y_true.max(), lgb_preds_oof.max(), xgb_preds_oof.max(), ens_preds_oof.max())\n\n    plt.figure(figsize=(15, 5))\n\n    # LightGBM OOF\n    plt.subplot(1, 3, 1)\n    sns.scatterplot(x=lgb_preds_oof, y=y_true, alpha=0.5, color='blue')\n    plt.plot([line_min, line_max], [line_min, line_max], 'r--')\n    plt.title(f'{t} OOF LightGBM')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n\n    # XGBoost OOF\n    plt.subplot(1, 3, 2)\n    sns.scatterplot(x=xgb_preds_oof, y=y_true, alpha=0.5, color='green')\n    plt.plot([line_min, line_max], [line_min, line_max], 'r--')\n    plt.title(f'{t} OOF XGBoost')\n    plt.xlabel('Predicted')\n\n    # Ensemble OOF\n    plt.subplot(1, 3, 3)\n    sns.scatterplot(x=ens_preds_oof, y=y_true, alpha=0.5, color='orange')\n    plt.plot([line_min, line_max], [line_min, line_max], 'r--')\n    plt.title(f'{t} OOF Ensemble')\n    plt.xlabel('Predicted')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MINMAX_DICT =  {\n        'Tg': [-148.0297376, 472.25],\n        'FFV': [0.2269924, 0.77709707],\n        'Tc': [0.0465, 0.524],\n        'Density': [0.748691234, 1.840998909],\n        'Rg': [9.7283551, 34.672905605],\n    }\nNULL_FOR_SUBMISSION = -9999\n\ndef scaling_error(labels, preds, property):\n    error = np.abs(labels - preds)\n    min_val, max_val = MINMAX_DICT[property]\n    label_range = max_val - min_val\n    return np.mean(error / label_range)\n\ndef get_property_weights(labels):\n    property_weight = []\n    for property in MINMAX_DICT.keys():\n        valid_num = np.sum(labels[property] != NULL_FOR_SUBMISSION)\n        property_weight.append(valid_num)\n    property_weight = np.array(property_weight)\n    property_weight = np.sqrt(1 / property_weight)\n    return (property_weight / np.sum(property_weight)) * len(property_weight)\n\ndef wmae_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    chemical_properties = list(MINMAX_DICT.keys())\n    property_maes = []\n    property_weights = get_property_weights(solution[chemical_properties])\n    for property in chemical_properties:\n        is_labeled = solution[property] != NULL_FOR_SUBMISSION\n        property_maes.append(scaling_error(solution.loc[is_labeled, property], submission.loc[is_labeled, property], property))\n\n    if len(property_maes) == 0:\n        raise RuntimeError('No labels')\n    return float(np.average(property_maes, weights=property_weights))\n\nfor target in CFG.TARGETS:\n    train[f'{target}_pred'] = np.nan  # Î®ºÏ†Ä Ï¥àÍ∏∞Ìôî\n    nonnull_idx = train[train[target].notnull()].index\n    train.loc[nonnull_idx, f'{target}_pred'] = oof_results[target]\n\ntr_solution = train[['id'] + CFG.TARGETS]\ntr_submission = train[['id'] + [t + '_pred' for t in CFG.TARGETS]]\ntr_submission.columns = ['id'] + CFG.TARGETS\nprint(f\"wMAE: {round(wmae_score(tr_solution, tr_submission, row_id_column_name='id'), 5)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**If you don't use a direct check for the presence of SMILES in the test data from external data, the LB result will be `0.47`**","metadata":{}},{"cell_type":"code","source":"for target in CFG.TARGETS:\n    test[target] = test_results[target]\nfor t in CFG.TARGETS:\n    for s in train[train[t].notnull()]['SMILES']:\n        if s in test['SMILES'].tolist():\n            test.loc[test['SMILES']==s, t] = train[train['SMILES']==s][t].values[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I'm rather disappointed that we are faced with the following situation, that test data contains data from publicly available sources is terrible. Also only the organizer can determine exactly where there is a license and where there is not. I'm not particularly interested in the cash prize, and I would like to make a good solution, but with only organizer's data it is difficult. I hope that the situation with the data used will become clearer.","metadata":{}},{"cell_type":"code","source":"\ntest[['id'] + CFG.TARGETS].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}