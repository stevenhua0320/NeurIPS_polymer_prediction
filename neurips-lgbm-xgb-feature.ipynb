{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":12207625,"sourceType":"datasetVersion","datasetId":7690162},{"sourceId":12330396,"sourceType":"datasetVersion","datasetId":7709869},{"sourceId":248128822,"sourceType":"kernelVersion"},{"sourceId":458385,"sourceType":"modelInstanceVersion","modelInstanceId":371577,"modelId":392477}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:42:45.770929Z","iopub.execute_input":"2025-07-09T07:42:45.771293Z","iopub.status.idle":"2025-07-09T07:42:49.822304Z","shell.execute_reply.started":"2025-07-09T07:42:45.771268Z","shell.execute_reply":"2025-07-09T07:42:49.82109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers rdkit pandas tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:42:49.824477Z","iopub.execute_input":"2025-07-09T07:42:49.824809Z","iopub.status.idle":"2025-07-09T07:42:53.76226Z","shell.execute_reply.started":"2025-07-09T07:42:49.824778Z","shell.execute_reply":"2025-07-09T07:42:53.760991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom transformers import AutoTokenizer, AutoModel\nimport gc\nimport pickle\nimport torch\nfrom tqdm import tqdm\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\n\nimport networkx as nx\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem import Descriptors\nfrom rdkit.Chem import rdmolops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:42:53.763852Z","iopub.execute_input":"2025-07-09T07:42:53.76485Z","iopub.status.idle":"2025-07-09T07:42:53.772416Z","shell.execute_reply.started":"2025-07-09T07:42:53.764811Z","shell.execute_reply":"2025-07-09T07:42:53.771271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n    SEED = 42\n    FOLDS = 5\n    PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\n\ntrain = pd.read_csv(CFG.PATH + 'train.csv')\ntest = pd.read_csv(CFG.PATH + 'test.csv')\n\ndef make_smile_canonical(smile): # To avoid duplicates, for example: canonical '*C=C(*)C' == '*C(=C*)C'\n    try:\n        mol = Chem.MolFromSmiles(smile)\n        canon_smile = Chem.MolToSmiles(mol, canonical=True)\n        return canon_smile\n    except:\n        return np.nan\n\ntrain['SMILES'] = train['SMILES'].apply(lambda s: make_smile_canonical(s))\ntest['SMILES'] = test['SMILES'].apply(lambda s: make_smile_canonical(s))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:42:53.774565Z","iopub.execute_input":"2025-07-09T07:42:53.774861Z","iopub.status.idle":"2025-07-09T07:42:59.753713Z","shell.execute_reply.started":"2025-07-09T07:42:53.774838Z","shell.execute_reply":"2025-07-09T07:42:59.75278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://www.kaggle.com/datasets/minatoyukinaxlisa/tc-smiles\ndata_tc = pd.read_csv('/kaggle/input/tc-smiles/Tc_SMILES.csv')\ndata_tc = data_tc.rename(columns={'TC_mean': 'Tc'})\n\n# https://springernature.figshare.com/articles/dataset/dataset_with_glass_transition_temperature/24219958?file=42507037\ndata_tg2 = pd.read_csv('/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv', usecols=['SMILES', 'Tg (C)'])\ndata_tg2 = data_tg2.rename(columns={'Tg (C)': 'Tg'})\n\n# https://www.sciencedirect.com/science/article/pii/S2590159123000377#ec0005\ndata_tg3 = pd.read_excel('/kaggle/input/smiles-extra-data/data_tg3.xlsx')\ndata_tg3 = data_tg3.rename(columns={'Tg [K]': 'Tg'})\ndata_tg3['Tg'] = data_tg3['Tg'] - 273.15\n\n# https://github.com/Duke-MatSci/ChemProps\ndata_dnst = pd.read_excel('/kaggle/input/smiles-extra-data/data_dnst1.xlsx')\ndata_dnst = data_dnst.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\ndata_dnst['SMILES'] = data_dnst['SMILES'].apply(lambda s: make_smile_canonical(s))\ndata_dnst = data_dnst[(data_dnst['SMILES'].notnull())&(data_dnst['Density'].notnull())&(data_dnst['Density'] != 'nylon')]\ndata_dnst['Density'] = data_dnst['Density'].astype('float64')\ndata_dnst['Density'] -= 0.118\n\ndataset1 = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\")\ndataset3 = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\")\ndataset4 = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\")\ndataset1 = dataset1.rename(columns={'TC_mean': 'Tc'})\n\n\ndef add_extra_data(df_train, df_extra, target):\n    n_samples_before = len(df_train[df_train[target].notnull()])\n    \n    df_extra['SMILES'] = df_extra['SMILES'].apply(lambda s: make_smile_canonical(s))\n    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n\n    # Make priority target value from competition's df\n    for smile in df_train[df_train[target].notnull()]['SMILES'].tolist():\n        if smile in cross_smiles:\n            cross_smiles.remove(smile)\n\n    # Imput missing values for competition's SMILES\n    for smile in cross_smiles:\n        df_train.loc[df_train['SMILES']==smile, target] = df_extra[df_extra['SMILES']==smile][target].values[0]\n    \n    df_train = pd.concat([df_train, df_extra[df_extra['SMILES'].isin(unique_smiles_extra)]], axis=0).reset_index(drop=True)\n\n    n_samples_after = len(df_train[df_train[target].notnull()])\n    print(f'\\nFor target \"{target}\" added {n_samples_after-n_samples_before} new samples!')\n    print(f'New unique SMILES: {len(unique_smiles_extra)}')\n    return df_train\n\ntrain = add_extra_data(train, data_tc, 'Tc')\ntrain = add_extra_data(train, data_tg2, 'Tg')\ntrain = add_extra_data(train, data_tg3, 'Tg')\ntrain = add_extra_data(train, data_dnst, 'Density')\ntrain = add_extra_data(train, dataset1, 'Tc')\ntrain = add_extra_data(train, dataset3, 'Tg')\ntrain = add_extra_data(train, dataset4, 'FFV')\n\nprint('\\n'*3, '--- SMILES for training ---', )\nfor t in CFG.TARGETS:\n    print(f'\"{t}\": {len(train[train[t].notnull()])}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:42:59.755131Z","iopub.execute_input":"2025-07-09T07:42:59.755413Z","iopub.status.idle":"2025-07-09T07:43:02.773224Z","shell.execute_reply.started":"2025-07-09T07:42:59.755384Z","shell.execute_reply":"2025-07-09T07:43:02.772348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel_dir = \"/kaggle/input/bert_smile/pytorch/default/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModel.from_pretrained(model_dir)\n\nmodel.eval()  # 평가 모드로 설정 (드롭아웃 등 비활성화)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:43:02.774101Z","iopub.execute_input":"2025-07-09T07:43:02.774354Z","iopub.status.idle":"2025-07-09T07:43:02.862444Z","shell.execute_reply.started":"2025-07-09T07:43:02.774333Z","shell.execute_reply":"2025-07-09T07:43:02.86167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)  # 모델 GPU로\n\ndef smiles_to_embedding(smiles, tokenizer, model):\n    try:\n        # 입력을 GPU로\n        inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n        inputs = {key: val.to(device) for key, val in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        last_hidden = outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n        pooled = last_hidden.mean(dim=1).squeeze()  # [hidden_dim]\n        return pooled.cpu().numpy()  # GPU → CPU → NumPy\n    except Exception as e:\n        print(f\"Error in SMILES: {smiles} - {e}\")\n        return [0.0] * 768  # 에러 시 0 벡터\n\n# Train SMILES 임베딩\ntrain_embeddings = []\nfor smi in tqdm(train[\"SMILES\"], desc=\"Embedding train SMILES\"):\n    emb = smiles_to_embedding(smi, tokenizer, model)\n    train_embeddings.append(emb)\n\ntrain_emb_df = pd.DataFrame(train_embeddings, columns=[f\"smiles_emb_{i}\" for i in range(768)])\ntrain = pd.concat([train.reset_index(drop=True), train_emb_df], axis=1)\n\n# Test SMILES 임베딩\ntest_embeddings = []\nfor smi in tqdm(test[\"SMILES\"], desc=\"Embedding test SMILES\"):\n    emb = smiles_to_embedding(smi, tokenizer, model)\n    test_embeddings.append(emb)\n\ntest_emb_df = pd.DataFrame(test_embeddings, columns=[f\"smiles_emb_{i}\" for i in range(768)])\ntest = pd.concat([test.reset_index(drop=True), test_emb_df], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:43:02.863235Z","iopub.execute_input":"2025-07-09T07:43:02.863465Z","iopub.status.idle":"2025-07-09T07:51:44.442684Z","shell.execute_reply.started":"2025-07-09T07:43:02.863448Z","shell.execute_reply":"2025-07-09T07:51:44.441691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.decomposition import PCA\n\n# 1. SMILES 임베딩 컬럼 추출\nembedding_cols = [col for col in train.columns if col.startswith('smiles_emb_')]\n\n# 2. Train에서 PCA 학습\nX_emb_train = train[embedding_cols].values\npca = PCA(n_components=0.9, svd_solver='full')\nX_pca_train = pca.fit_transform(X_emb_train)\n\n# 3. Test에 PCA 적용\nX_emb_test = test[embedding_cols].values\nX_pca_test = pca.transform(X_emb_test)\n\n# 4. 컬럼 이름 생성\npca_cols = [f'pca_smiles_emb_{i}' for i in range(X_pca_train.shape[1])]\n\n# 5. DataFrame 생성\ndf_pca_train = pd.DataFrame(X_pca_train, columns=pca_cols, index=train.index)\ndf_pca_test = pd.DataFrame(X_pca_test, columns=pca_cols, index=test.index)\n\n# 6. 기존 임베딩 컬럼 제거\ntrain.drop(columns=embedding_cols, inplace=True)\ntest.drop(columns=embedding_cols, inplace=True)\n\n# 7. PCA 컬럼 추가\ntrain = pd.concat([train, df_pca_train], axis=1)\ntest = pd.concat([test, df_pca_test], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:51:44.443803Z","iopub.execute_input":"2025-07-09T07:51:44.444083Z","iopub.status.idle":"2025-07-09T07:51:45.259152Z","shell.execute_reply.started":"2025-07-09T07:51:44.444055Z","shell.execute_reply":"2025-07-09T07:51:45.25848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nuseless_cols = [\n    # Nan data\n    'BCUT2D_MWHI',\n    'BCUT2D_MWLOW',\n    'BCUT2D_CHGHI',\n    'BCUT2D_CHGLO',\n    'BCUT2D_LOGPHI',\n    'BCUT2D_LOGPLOW',\n    'BCUT2D_MRHI',\n    'BCUT2D_MRLOW',\n\n    # Constant data\n    'NumRadicalElectrons',\n    'SMR_VSA8',\n    'SlogP_VSA9',\n    'fr_barbitur',\n    'fr_benzodiazepine',\n    'fr_dihydropyridine',\n    'fr_epoxide',\n    'fr_isothiocyan',\n    'fr_lactam',\n    'fr_nitroso',\n    'fr_prisulfonamd',\n    'fr_thiocyan',\n\n    # High correlated data >0.95\n    'MaxEStateIndex',\n    'HeavyAtomMolWt',\n    'ExactMolWt',\n    'NumValenceElectrons',\n    'Chi0',\n    'Chi0n',\n    'Chi0v',\n    'Chi1',\n    'Chi1n',\n    'Chi1v',\n    'Chi2n',\n    'Kappa1',\n    'LabuteASA',\n    'HeavyAtomCount',\n    'MolMR',\n    'Chi3n',\n    'BertzCT',\n    'Chi2v',\n    'Chi4n',\n    'HallKierAlpha',\n    'Chi3v',\n    'Chi4v',\n    'MinAbsPartialCharge',\n    'MinPartialCharge',\n    'MaxAbsPartialCharge',\n    'FpDensityMorgan2',\n    'FpDensityMorgan3',\n    'Phi',\n    'Kappa3',\n    'fr_nitrile',\n    'SlogP_VSA6',\n    'NumAromaticCarbocycles',\n    'NumAromaticRings',\n    'fr_benzene',\n    'VSA_EState6',\n    'NOCount',\n    'fr_C_O',\n    'fr_C_O_noCOO',\n    'NumHDonors',\n    'fr_amide',\n    'fr_Nhpyrrole',\n    'fr_phenol',\n    'fr_phenol_noOrthoHbond',\n    'fr_COO2',\n    'fr_halogen',\n    'fr_diazo',\n    'fr_nitro_arom',\n    'fr_phos_ester',\n    #low shap\n    'NumBridgeheadAtoms', 'NumSaturatedCarbocycles', 'NumSaturatedRings',\n    'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA3', 'PEOE_VSA4',\n    'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_Ar_COO', 'fr_Ar_NH',\n    'fr_COO', 'fr_Imine', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2',\n    'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_amidine', 'fr_azo', 'fr_furan',\n    'fr_imidazole', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactone', 'fr_methoxy',\n    'fr_morpholine',  'fr_nitro_arom_nonortho', 'fr_piperdine',\n    'fr_priamide', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd',\n    'fr_sulfone', 'fr_thiazole', 'fr_urea'\n]\ndef count_atoms(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    counts = {'num_C': 0, 'num_c': 0, 'num_O': 0, 'num_N': 0, 'num_F': 0, 'num_Cl': 0}\n    if mol is None:\n        return counts\n\n    for atom in mol.GetAtoms():\n        symbol = atom.GetSymbol()\n        if symbol == 'C':\n            if atom.GetIsAromatic():\n                counts['num_c'] += 1\n            else:\n                counts['num_C'] += 1\n        elif symbol == 'Cl':\n            counts['num_Cl'] += 1\n        elif symbol in ['O', 'N', 'F']:\n            counts[f'num_{symbol}'] += 1\n    return counts\n\n\ndef compute_all_descriptors(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return [None] * len(desc_names)\n    return [desc[1](mol) for desc in Descriptors.descList if desc[0] not in useless_cols]\n\ndef compute_graph_features(smiles, graph_feats):\n    mol = Chem.MolFromSmiles(smiles)\n    adj = rdmolops.GetAdjacencyMatrix(mol)\n    G = nx.from_numpy_array(adj)\n\n    graph_feats['graph_diameter'].append(nx.diameter(G) if nx.is_connected(G) else 0)\n    graph_feats['avg_shortest_path'].append(nx.average_shortest_path_length(G) if nx.is_connected(G) else 0)\n    graph_feats['num_cycles'].append(len(list(nx.cycle_basis(G))))\n\ndef preprocessing(df):\n    desc_names = [desc[0] for desc in Descriptors.descList if desc[0] not in useless_cols]\n    descriptors = [compute_all_descriptors(smi) for smi in df['SMILES'].to_list()]\n\n    # 그래프 특성 추출\n    graph_feats = {'graph_diameter': [], 'avg_shortest_path': [], 'num_cycles': []}\n    for smile in df['SMILES']:\n         compute_graph_features(smile, graph_feats)\n\n    # 원자 개수 계산\n    atom_counts = [count_atoms(smi) for smi in df['SMILES']]\n    atom_df = pd.DataFrame(atom_counts)\n    # 결합\n    result = pd.concat(\n        [\n            pd.DataFrame(descriptors, columns=desc_names),\n            pd.DataFrame(graph_feats),\n            atom_df\n        ],\n        axis=1\n    )\n    # 후처리 및 파생변수\n    result = result.replace([-np.inf, np.inf], np.nan)\n    eps = 1e-6\n\n    result[\"MaxPartialCharge_mul_EState_VSA6\"] = result[\"MaxPartialCharge\"] * result[\"EState_VSA6\"]\n    result[\"MaxPartialCharge_mul_SMR_VSA9\"] = result[\"MaxPartialCharge\"] * result[\"SMR_VSA9\"]\n    result[\"MaxPartialCharge_mul_RingCount\"] = result[\"MaxPartialCharge\"] * result[\"RingCount\"]\n    result[\"MaxPartialCharge_mul_num_cycles\"] = result[\"MaxPartialCharge\"] * result[\"num_cycles\"]\n    result[\"MaxPartialCharge_div_NumHAcceptors\"] = result[\"MaxPartialCharge\"] / (result[\"NumHAcceptors\"] + eps)\n    result[\"MaxPartialCharge_div_MinEStateIndex\"] = result[\"MaxPartialCharge\"] / (result[\"MinEStateIndex\"] + eps)\n    result[\"MaxPartialCharge_div_SlogP_VSA12\"] = result[\"MaxPartialCharge\"] / (result[\"SlogP_VSA12\"] + eps)\n    result[\"MaxPartialCharge_div_PEOE_VSA6\"] = result[\"MaxPartialCharge\"] / (result[\"PEOE_VSA6\"] + eps)\n    result[\"MaxPartialCharge_mul_SlogP_VSA10\"] = result[\"MaxPartialCharge\"] * result[\"SlogP_VSA10\"]\n    result[\"MaxPartialCharge_mul_AvgIpc\"] = result[\"MaxPartialCharge\"] * result[\"AvgIpc\"]\n    result[\"NumAromaticHeterocycles_div_NumHeteroatoms\"]=result[\"NumAromaticHeterocycles\"]/(result[\"NumHeteroatoms\"]+eps)\n    result[\"fr_unbrch_alkane_div_MolWt \"]=result[\"fr_unbrch_alkane\"]/(result[\"MolWt\"]+eps)\n    result[\"fr_ester_div_MaxPartialCharge\"] = result[\"fr_ester\"] / (result[\"MaxPartialCharge\"] + eps)\n    result[\"avg_shortest_path_div_MaxPartialCharge\"] = result[\"avg_shortest_path\"] / (result[\"MaxPartialCharge\"] + eps)\n    result[\"graph_diameter_div_MaxPartialCharge\"] = result[\"graph_diameter\"] / (result[\"MaxPartialCharge\"] + eps)\n    result[\"VSA_EState8_div_MaxPartialCharge\"] = result[\"VSA_EState8\"] / (result[\"MaxPartialCharge\"] + eps)\n    result[\"EState_VSA5_div_MaxPartialCharge\"] = result[\"EState_VSA5\"] / (result[\"MaxPartialCharge\"] + eps)\n    result[\"PEOE_VSA14_div_graph_diameter\"] = result[\"PEOE_VSA14\"] / (result[\"graph_diameter\"] + eps)\n    result[\"BalabanJ_mul_TPSA\"] = result[\"BalabanJ\"] * result[\"TPSA\"]\n    result[\"qed_mul_SMR_VSA5\"] = result[\"qed\"] * result[\"SMR_VSA5\"]\n    result[\"VSA_EState7_div_MolWt\"] = result[\"VSA_EState7\"] / (result[\"MolWt\"] + eps)\n    result[\"SMR_VSA10_div_MolWt\"] = result[\"SMR_VSA10\"] / (result[\"MolWt\"] + eps)\n    result[\"SlogP_VSA12_div_MolWt\"] = result[\"SlogP_VSA12\"] / (result[\"MolWt\"] + eps)\n    result[\"SMR_VSA10_div_fr_unbrch_alkane\"] = result[\"SMR_VSA10\"] / (result[\"fr_unbrch_alkane\"] + eps)\n    result[\"qed_mul_TPSA\"] = result[\"qed\"] * result[\"TPSA\"]\n    result[\"PEOE_VSA14_div_fr_unbrch_alkane\"] = result[\"PEOE_VSA14\"] / (result[\"fr_unbrch_alkane\"] + eps)\n    result[\"PEOE_VSA14_mul_AvgIpc\"] = result[\"PEOE_VSA14\"] * result[\"AvgIpc\"]\n    result[\"SMR_VSA5_div_MolWt\"] = result[\"SMR_VSA5\"] / (result[\"MolWt\"] + eps)\n    result[\"PEOE_VSA14_div_SlogP_VSA7\"] = result[\"PEOE_VSA14\"] / (result[\"SlogP_VSA7\"] + eps)\n    result[\"VSA_EState7_div_SPS\"] = result[\"VSA_EState7\"] / (result[\"SPS\"] + eps)\n    result[\"SlogP_VSA5_mul_FpDensityMorgan1\"] = result[\"SlogP_VSA5\"] * result[\"FpDensityMorgan1\"]\n    result[\"VSA_EState8_div_PEOE_VSA5\"] = result[\"VSA_EState8\"] / (result[\"PEOE_VSA5\"] + eps)\n    result['MaxPartialCharge_mul_num_cycles_div_fr_oxazole'] = (\n        result['MaxPartialCharge'] * result['num_cycles']\n    ) / (result['fr_oxazole'] + eps)\n    result['SMR_VSA5_div_MolWt_div_fr_nitro'] = (\n        result['SMR_VSA5'] / (result['MolWt'] + eps)\n    ) / (result['fr_nitro'] + eps)\n    result['fr_unbrch_alkane_div_MolWt_div_EState_VSA11'] = (\n        result['fr_unbrch_alkane'] / (result['MolWt'] + eps)\n    ) / (result['EState_VSA11'] + eps)\n    result['PEOE_VSA14_div_graph_diameter_div_BalabanJ'] = (\n        result['PEOE_VSA14'] / (result['graph_diameter'] + eps)\n    ) / (result['BalabanJ'] + eps)\n    result['VSA_EState7_div_SPS_div_PEOE_VSA14'] = (\n        result['VSA_EState7'] / (result['SPS'] + eps)\n    ) / (result['PEOE_VSA14'] + eps)\n    return result\n\ntrain = pd.concat([train, preprocessing(train)], axis=1)\ntest = pd.concat([test, preprocessing(test)], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T07:57:12.245277Z","iopub.execute_input":"2025-07-09T07:57:12.245674Z","iopub.status.idle":"2025-07-09T08:00:07.19757Z","shell.execute_reply.started":"2025-07-09T07:57:12.245648Z","shell.execute_reply":"2025-07-09T08:00:07.196256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_features = train.columns[7:].tolist()\nfeatures = {}\nfor target in CFG.TARGETS:\n    const_descs = []\n    for col in train.columns.drop(CFG.TARGETS):\n        if train.loc[train[target].notnull(), col].nunique() == 1:\n            const_descs.append(col)\n    features[target] = [f for f in all_features if f not in const_descs]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T08:00:07.199574Z","iopub.execute_input":"2025-07-09T08:00:07.19997Z","iopub.status.idle":"2025-07-09T08:00:07.708846Z","shell.execute_reply.started":"2025-07-09T08:00:07.19994Z","shell.execute_reply":"2025-07-09T08:00:07.706042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T08:00:07.710289Z","iopub.execute_input":"2025-07-09T08:00:07.710662Z","iopub.status.idle":"2025-07-09T08:00:07.717094Z","shell.execute_reply.started":"2025-07-09T08:00:07.710607Z","shell.execute_reply":"2025-07-09T08:00:07.716173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom lightgbm import early_stopping, reset_parameter\n\nlast_model_lgb_dict = {}\nlast_model_xgb_dict = {}\n\n# 러닝레이트 디케이 함수\ndef lr_decay(current_round, total_rounds=10000, start=0.02, end=0.005):\n    lr = start - (start - end) * (current_round / total_rounds)\n    return max(lr, end)\n\ndef lgb_lr_decay(current_round, target):\n    if target == 'FFV':\n        start = 0.02\n        end = 0.005\n        total = 10000\n        lr = start - (start - end) * (current_round / total)\n        return max(lr, end)\n    else:\n        return 0.01\n\n# MAE 계산 함수\ndef mae(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))\n\n# 최적 가중치 탐색 함수\ndef find_best_weight(y_true, pred_lgb, pred_xgb):\n    best_mae = float('inf')\n    best_weight = 0.5\n    for w in np.linspace(0, 1, 101):\n        blended = w * pred_lgb + (1 - w) * pred_xgb\n        current_mae = mae(y_true, blended)\n        if current_mae < best_mae:\n            best_mae = current_mae\n            best_weight = w\n    return best_weight, best_mae\n\n# 하이퍼파라미터 설정\nbase_params = {\n    'device_type': 'cpu',\n    'n_estimators': 1000000,\n    'objective': 'regression_l1',\n    'metric': 'mae',\n    'verbosity': -1,\n    'num_leaves': 30,\n    'min_data_in_leaf': 2,\n    'learning_rate': 0.01,  # 기본값, 실제 러닝레이트는 callback에서 조절\n    'max_bin': 512,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n    'lambda_l1': 2,\n    'lambda_l2': 3,\n}\n\nxgb_params = {\n    #'tree_method': 'gpu_hist',\n    'n_estimators': 1000000,\n    'objective': 'reg:squarederror',\n    'learning_rate': 0.01,\n    'max_depth': 4,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 1,\n    'verbosity': 0,\n}\n\n# 결과 저장용 dict\noof_results = {}\ntest_results = {}\nensemble_weights = {}\n\noof_lgb_dict = {}\noof_xgb_dict = {}\noof_ens_dict = {}\n\n# 결과 저장 dict 초기화\noof_results = {}\ntest_results = {}\nensemble_weights = {}\n\noof_lgb_dict = {}\noof_xgb_dict = {}\noof_ens_dict = {}\n\nfor target in CFG.TARGETS:\n    print(f'\\n▶ TARGET: {target}')\n    \n    train_part = train[train[target].notnull()].reset_index(drop=True)\n    \n    \n    test[target] = 0\n    features_list = features[target]\n\n    oof_lgb = np.zeros(len(train_part))\n    oof_xgb = np.zeros(len(train_part))\n    test_preds_lgb = np.zeros(len(test))\n    test_preds_xgb = np.zeros(len(test))\n    scores_lgb, scores_xgb = [], []\n\n    kf = KFold(n_splits=CFG.FOLDS, shuffle=True, random_state=CFG.SEED)\n\n    last_model_lgb = None\n    last_model_xgb = None\n\n    for i, (trn_idx, val_idx) in enumerate(kf.split(train_part, train_part[target])):\n        print(f'\\n--- Fold {i+1} ---')\n        X_trn = train_part.loc[trn_idx, features_list]\n        y_trn = train_part.loc[trn_idx, target]\n        X_val = train_part.loc[val_idx, features_list]\n        y_val = train_part.loc[val_idx, target]\n\n        X_trn = np.clip(X_trn, -1e6, 1e6)\n        X_val = np.clip(X_val, -1e6, 1e6)\n\n        # LightGBM\n        model_lgb = lgb.LGBMRegressor(**base_params)\n        callbacks = [\n            lgb.early_stopping(300, verbose=False),\n            lgb.reset_parameter(learning_rate=lambda cur_round: lgb_lr_decay(cur_round, target))\n        ]\n        model_lgb.fit(\n            X_trn, y_trn,\n            eval_set=[(X_val, y_val)],\n            callbacks=callbacks,\n       \n        )\n        val_preds_lgb = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration_)\n        print(f'LightGBM Fold {i+1} MAE: {mae(y_val, val_preds_lgb):.5f}')\n        oof_lgb[val_idx] = val_preds_lgb\n        test_preds_lgb += model_lgb.predict(test[features_list]) / CFG.FOLDS\n        scores_lgb.append(mae(y_val, val_preds_lgb))\n\n        # 마지막 fold LightGBM 모델 저장\n        if i == CFG.FOLDS - 1:\n            last_model_lgb_dict[target] = model_lgb\n\n        # XGBoost\n        xgb_callbacks = []\n        if target == 'FFV':\n            xgb_callbacks = [xgb.callback.LearningRateScheduler(lambda r: lgb_lr_decay(r, target))]\n        model_xgb = xgb.XGBRegressor(**xgb_params)\n        model_xgb.fit(\n            X_trn, y_trn,\n            eval_set=[(X_val, y_val)],\n            early_stopping_rounds=300,\n            verbose=False,\n            callbacks=xgb_callbacks\n        )\n        val_preds_xgb = model_xgb.predict(X_val, iteration_range=(0, model_xgb.best_iteration))\n        print(f'XGBoost Fold {i+1} MAE: {mae(y_val, val_preds_xgb):.5f}')\n        oof_xgb[val_idx] = val_preds_xgb\n        test_preds_xgb += model_xgb.predict(test[features_list]) / CFG.FOLDS\n        scores_xgb.append(mae(y_val, val_preds_xgb))\n\n        # 마지막 fold XGBoost 모델 저장\n        if i == CFG.FOLDS - 1:\n            last_model_xgb_dict[target] = model_xgb\n\n    y_true = train_part[target].values\n    best_w, best_mae = find_best_weight(y_true, oof_lgb, oof_xgb)\n    ensemble_weights[target] = best_w\n\n    oof_blend = best_w * oof_lgb + (1 - best_w) * oof_xgb\n    test_blend = best_w * test_preds_lgb + (1 - best_w) * test_preds_xgb\n\n    oof_lgb_dict[target] = oof_lgb\n    oof_xgb_dict[target] = oof_xgb\n    oof_ens_dict[target] = oof_blend\n\n    oof_results[target] = oof_blend\n    test_results[target] = test_blend\n\n    print(f'\\n📌 LightGBM MAE: {np.mean(scores_lgb):.5f}')\n    print(f'📌 XGBoost  MAE: {np.mean(scores_xgb):.5f}')\n    print(f'🔍 Best Weight (LGB): {best_w:.2f} → Blended MAE: {best_mae:.5f}')\n    print('-'*40)\n\n# OOF 및 테스트 결과 저장\noof_df = pd.DataFrame({'id': train['id']})\ntest_df = pd.DataFrame({'id': test['id']})\n\nfor target in CFG.TARGETS:\n    oof_col = np.full(len(train), np.nan)\n    nonnull_idx = train[train[target].notnull()].index\n    oof_col[nonnull_idx] = oof_results[target]\n\n    oof_df[target] = oof_col\n    test_df[target] = test_results[target]\n\noof_df.to_csv('oof_ensemble.csv', index=False)\ntest_df.to_csv('test_ensemble.csv', index=False)\n\nprint('\\n✅ 앙상블 완료 및 파일 저장 완료')\n\n\n\"\"\"\n▶ TARGET: Tg\n📌 LightGBM MAE: 36.31066\n📌 XGBoost  MAE: 35.90509\n🔍 Best Weight (LGB): 0.39 → Blended MAE: 35.73759\n----------------------------------------\n\n\n▶ TARGET: FFV\n📌 LightGBM MAE: 0.00648\n📌 XGBoost  MAE: 0.00929\n🔍 Best Weight (LGB): 1.00 → Blended MAE: 0.00648\n----------------------------------------\n\n\n▶ TARGET: Tc\n📌 LightGBM MAE: 0.03016\n📌 XGBoost  MAE: 0.03349\n🔍 Best Weight (LGB): 1.00 → Blended MAE: 0.03015\n----------------------------------------\n\n\n▶ TARGET: Density\n📌 LightGBM MAE: 0.03904\n📌 XGBoost  MAE: 0.04353\n🔍 Best Weight (LGB): 0.94 → Blended MAE: 0.03902\n----------------------------------------\n\n\n▶ TARGET: Rg\n📌 LightGBM MAE: 1.69317\n📌 XGBoost  MAE: 1.66246\n🔍 Best Weight (LGB): 0.06 → Blended MAE: 1.66245\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T08:00:07.720134Z","iopub.execute_input":"2025-07-09T08:00:07.720543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import shap\nimport numpy as np\n\ncommon_low_features_all_targets = None  # 모든 target 공통 하위 피처 교집합 저장용\n\nfor target in CFG.TARGETS:\n    print(f\"\\n🔍 SHAP 분석 시작: {target}\")\n\n    train_part = train[train[target].notnull()].reset_index(drop=True)\n    features_list = features[target]\n    X_sample = train_part[features_list].copy()\n    X_sample = X_sample.replace([np.inf, -np.inf], np.nan).fillna(0)\n    X_sample = np.clip(X_sample, -1e6, 1e6)\n\n    model_lgb = last_model_lgb_dict[target]\n    explainer_lgb = shap.Explainer(model_lgb)\n    shap_values_lgb = explainer_lgb(X_sample)\n    shap_mean_lgb = np.abs(shap_values_lgb.values).mean(axis=0)\n\n    model_xgb = last_model_xgb_dict[target]\n    explainer_xgb = shap.Explainer(model_xgb)\n    shap_values_xgb = explainer_xgb(X_sample)\n    shap_mean_xgb = np.abs(shap_values_xgb.values).mean(axis=0)\n\n    low_idx_lgb = np.argsort(shap_mean_lgb)[:100]\n    low_idx_xgb = np.argsort(shap_mean_xgb)[:100]\n\n    common_low_idx = np.intersect1d(low_idx_lgb, low_idx_xgb)\n    common_low_features = np.array(features_list)[common_low_idx]\n\n    print(f\"{target} 공통 하위 100개 피처 수: {len(common_low_features)}\")\n\n    if common_low_features_all_targets is None:\n        common_low_features_all_targets = set(common_low_features)\n    else:\n        common_low_features_all_targets = common_low_features_all_targets.intersection(common_low_features)\n\nprint(f\"\\n=== 모든 target에서 공통으로 하위 100개에 포함된 피처 목록 (개수: {len(common_low_features_all_targets)}) ===\")\nprint(sorted(common_low_features_all_targets))\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor t in CFG.TARGETS:\n    y_true = train[train[t].notnull()][t].values\n    \n    # 각각 모델의 OOF 예측 (각각 딕셔너리로 저장했다고 가정)\n    lgb_preds_oof = oof_lgb_dict[t]\n    xgb_preds_oof = oof_xgb_dict[t]\n    ens_preds_oof = oof_ens_dict[t]\n\n    line_min = min(y_true.min(), lgb_preds_oof.min(), xgb_preds_oof.min(), ens_preds_oof.min())\n    line_max = max(y_true.max(), lgb_preds_oof.max(), xgb_preds_oof.max(), ens_preds_oof.max())\n\n    plt.figure(figsize=(15, 5))\n\n    # LightGBM OOF\n    plt.subplot(1, 3, 1)\n    sns.scatterplot(x=lgb_preds_oof, y=y_true, alpha=0.5, color='blue')\n    plt.plot([line_min, line_max], [line_min, line_max], 'r--')\n    plt.title(f'{t} OOF LightGBM')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n\n    # XGBoost OOF\n    plt.subplot(1, 3, 2)\n    sns.scatterplot(x=xgb_preds_oof, y=y_true, alpha=0.5, color='green')\n    plt.plot([line_min, line_max], [line_min, line_max], 'r--')\n    plt.title(f'{t} OOF XGBoost')\n    plt.xlabel('Predicted')\n\n    # Ensemble OOF\n    plt.subplot(1, 3, 3)\n    sns.scatterplot(x=ens_preds_oof, y=y_true, alpha=0.5, color='orange')\n    plt.plot([line_min, line_max], [line_min, line_max], 'r--')\n    plt.title(f'{t} OOF Ensemble')\n    plt.xlabel('Predicted')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MINMAX_DICT =  {\n        'Tg': [-148.0297376, 472.25],\n        'FFV': [0.2269924, 0.77709707],\n        'Tc': [0.0465, 0.524],\n        'Density': [0.748691234, 1.840998909],\n        'Rg': [9.7283551, 34.672905605],\n    }\nNULL_FOR_SUBMISSION = -9999\n\ndef scaling_error(labels, preds, property):\n    error = np.abs(labels - preds)\n    min_val, max_val = MINMAX_DICT[property]\n    label_range = max_val - min_val\n    return np.mean(error / label_range)\n\ndef get_property_weights(labels):\n    property_weight = []\n    for property in MINMAX_DICT.keys():\n        valid_num = np.sum(labels[property] != NULL_FOR_SUBMISSION)\n        property_weight.append(valid_num)\n    property_weight = np.array(property_weight)\n    property_weight = np.sqrt(1 / property_weight)\n    return (property_weight / np.sum(property_weight)) * len(property_weight)\n\ndef wmae_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    chemical_properties = list(MINMAX_DICT.keys())\n    property_maes = []\n    property_weights = get_property_weights(solution[chemical_properties])\n    for property in chemical_properties:\n        is_labeled = solution[property] != NULL_FOR_SUBMISSION\n        property_maes.append(scaling_error(solution.loc[is_labeled, property], submission.loc[is_labeled, property], property))\n\n    if len(property_maes) == 0:\n        raise RuntimeError('No labels')\n    return float(np.average(property_maes, weights=property_weights))\n\nfor target in CFG.TARGETS:\n    train[f'{target}_pred'] = np.nan  # 먼저 초기화\n    nonnull_idx = train[train[target].notnull()].index\n    train.loc[nonnull_idx, f'{target}_pred'] = oof_results[target]\n\ntr_solution = train[['id'] + CFG.TARGETS]\ntr_submission = train[['id'] + [t + '_pred' for t in CFG.TARGETS]]\ntr_submission.columns = ['id'] + CFG.TARGETS\nprint(f\"wMAE: {round(wmae_score(tr_solution, tr_submission, row_id_column_name='id'), 5)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**If you don't use a direct check for the presence of SMILES in the test data from external data, the LB result will be `0.47`**","metadata":{}},{"cell_type":"code","source":"for target in CFG.TARGETS:\n    test[target] = test_results[target]\nfor t in CFG.TARGETS:\n    for s in train[train[t].notnull()]['SMILES']:\n        if s in test['SMILES'].tolist():\n            test.loc[test['SMILES']==s, t] = train[train['SMILES']==s][t].values[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I'm rather disappointed that we are faced with the following situation, that test data contains data from publicly available sources is terrible. Also only the organizer can determine exactly where there is a license and where there is not. I'm not particularly interested in the cash prize, and I would like to make a good solution, but with only organizer's data it is difficult. I hope that the situation with the data used will become clearer.","metadata":{}},{"cell_type":"code","source":"\ntest[['id'] + CFG.TARGETS].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}